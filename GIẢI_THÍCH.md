# ğŸ•·ï¸ GIáº¢I THÃCH WEB SCRAPING Äá» N GIáº¢N

## ğŸ“– Code cá»§a báº¡n lÃ m gÃ¬?

```python
from bs4 import BeautifulSoup
import requests
```
**Nháº­p thÆ° viá»‡n:**
- `requests`: Gá»­i yÃªu cáº§u Ä‘áº¿n website Ä‘á»ƒ láº¥y HTML
- `BeautifulSoup`: PhÃ¢n tÃ­ch HTML Ä‘á»ƒ tÃ¬m thÃ´ng tin cáº§n thiáº¿t

---

## ğŸ” CÃ¡c bÆ°á»›c chÃ­nh

### BÆ°á»›c 1: Láº¥y HTML tá»« website
```python
response = requests.get(url, timeout=10)
```
- Gá»­i request Ä‘áº¿n `https://www.python.org/jobs/`
- Giá»‘ng nhÆ° báº¡n má»Ÿ website trÃªn trÃ¬nh duyá»‡t
- `timeout=10`: Chá» tá»‘i Ä‘a 10 giÃ¢y

### BÆ°á»›c 2: Parse (phÃ¢n tÃ­ch) HTML
```python
soup = BeautifulSoup(response.text, "html.parser")
```
- Chuyá»ƒn HTML thÃ´ thÃ nh cáº¥u trÃºc dá»… tÃ¬m kiáº¿m
- Giá»‘ng nhÆ° "Ä‘á»c hiá»ƒu" trang web

### BÆ°á»›c 3: TÃ¬m cÃ¡c elements cáº§n thiáº¿t
```python
job_posts = soup.find_all("h2", class_="listing-company")
```
- TÃ¬m Táº¤T Cáº¢ tháº» `<h2>` cÃ³ `class="listing-company"`
- Tráº£ vá» danh sÃ¡ch cÃ¡c elements tÃ¬m tháº¥y

### BÆ°á»›c 4: Láº¥y dá»¯ liá»‡u
```python
link = job_post.find("a")
if link:
    company_name = link.text.strip()
    print(company_name)
```
- TÃ¬m tháº» `<a>` (link) trong má»—i job post
- Láº¥y text bÃªn trong
- `.strip()`: XÃ³a khoáº£ng tráº¯ng thá»«a

---

## âš ï¸ CÃ¡c Ä‘iá»ƒm quan trá»ng

### 1. LuÃ´n check `None`
```python
# âŒ SAI - Sáº½ crash náº¿u khÃ´ng tÃ¬m tháº¥y
company = job_post.find("a").text

# âœ… ÄÃšNG - An toÃ n
link = job_post.find("a")
if link:
    company = link.text
```

### 2. Xá»­ lÃ½ lá»—i
```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()  # Kiá»ƒm tra status code
except requests.exceptions.Timeout:
    print("Timeout!")
except requests.exceptions.ConnectionError:
    print("KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c!")
```

### 3. Clean dá»¯ liá»‡u
```python
text = link.text.strip()  # XÃ³a spaces, \n, \t
```

---

## ğŸ¯ TÃ³m táº¯t nhanh

```
1. requests.get()        â†’ Láº¥y HTML tá»« website
2. BeautifulSoup()       â†’ Parse HTML
3. soup.find_all()       â†’ TÃ¬m elements
4. element.text          â†’ Láº¥y text
5. .strip()              â†’ Clean text
```

---

## ğŸ› ï¸ CÃ¡c method hay dÃ¹ng

### TÃ¬m kiáº¿m
```python
soup.find("h1")                          # TÃ¬m 1 cÃ¡i Ä‘áº§u tiÃªn
soup.find_all("h2")                      # TÃ¬m táº¥t cáº£
soup.find("div", class_="name")          # TÃ¬m theo class
soup.find("div", id="header")            # TÃ¬m theo id
soup.find("a", href="/jobs")             # TÃ¬m theo attribute
```

### Láº¥y dá»¯ liá»‡u
```python
element.text                    # Láº¥y text
element.get("href")            # Láº¥y attribute
element["href"]                # CÃ¡ch khÃ¡c
element.find("a")              # TÃ¬m con
element.find_next("p")         # TÃ¬m element tiáº¿p theo
```

---

## ğŸ’¡ Tips

1. **Inspect HTML trÆ°á»›c** (F12 trÃªn browser)
2. **Test vá»›i 1 item** trÆ°á»›c khi loop
3. **LuÃ´n check None** trÆ°á»›c khi access
4. **ThÃªm delay** náº¿u scrape nhiá»u pages: `time.sleep(1)`
5. **Äá»c robots.txt**: `website.com/robots.txt`

---

## âœ… Checklist khi scrape

- [ ] ThÃªm `timeout` cho requests
- [ ] DÃ¹ng `try-except` Ä‘á»ƒ handle lá»—i
- [ ] Check `None` trÆ°á»›c khi `.text` hoáº·c `.get()`
- [ ] DÃ¹ng `.strip()` Ä‘á»ƒ clean text
- [ ] Test vá»›i vÃ i items trÆ°á»›c
- [ ] ThÃªm delay giá»¯a requests (náº¿u nhiá»u)
- [ ] Kiá»ƒm tra robots.txt

---

## ğŸš€ NÃ¢ng cao (náº¿u cáº§n)

### LÆ°u vÃ o file
```python
with open("results.txt", "w", encoding="utf-8") as f:
    f.write(company_name + "\n")
```

### ThÃªm User-Agent (trÃ¡nh bá»‹ block)
```python
headers = {"User-Agent": "Mozilla/5.0 ..."}
response = requests.get(url, headers=headers)
```

### Delay giá»¯a requests
```python
import time
time.sleep(1)  # Chá» 1 giÃ¢y
```

---

**Váº­y thÃ´i! ÄÆ¡n giáº£n mÃ ! ğŸ‰**